# Logistic Regression from Scratch using NumPy ğŸ¤–
A complete implementation of Logistic Regression with Gradient Descent optimization from scratch using only NumPy, demonstrating mathematical foundations of binary classification for diabetes prediction.

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [Dataset](#dataset)
- [Features](#features)
- [Mathematical Foundation](#mathematical-foundation)
- [Installation & Usage](#installation--usage)
- [Project Structure](#project-structure)
- [Results](#results)
- [What I Learned](#what-i-learned)
- [Visualizations](#visualizations)

## ğŸ¯ Project Overview
This project implements Logistic Regression from scratch without using scikit-learn or Keras. It covers the complete ML pipeline:

- **Data Loading & Preprocessing** - Load diabetes dataset and handle missing/inconsistent values
- **Exploratory Data Analysis** - Understand feature distributions and class balance
- **Feature Engineering** - Encode categorical variables and scale features
- **Model Implementation** - Build logistic regression classifier using object-oriented design
- **Training with Gradient Descent** - Optimize weights and bias using cross-entropy loss
- **Evaluation & Visualization** - Assess performance with accuracy and loss curves

The goal is to understand how logistic regression actually works at a mathematical and computational level for binary classification tasks like disease prediction.

## ğŸ“Š Dataset

### Dataset: Diabetes Prediction Dataset

| Attribute | Details |
|-----------|---------|
| **Size** | Large dataset with diabetes records |
| **Features** | Multiple health-related features |
| **Target** | Diabetes (0 = No, 1 = Yes) |
| **Task** | Binary Classification |
| **Preprocessing** | Handled missing values, encoded categorical features, standardized numerical features |

**Key Columns:**
- Health metrics and vital signs
- Medical history indicators
- Target: Diabetes diagnosis (binary)

## âœ¨ Features

âœ… **From-Scratch Implementation** - No scikit-learn, only NumPy  
âœ… **Object-Oriented Design** - Reusable `LogisticRegression` class  
âœ… **Gradient Descent Optimization** - Iterative weight/bias updates  
âœ… **Cross-Entropy Loss** - Standard loss function for binary classification  
âœ… **Feature Scaling** - Standardization for faster convergence  
âœ… **Multiple Evaluation Metrics** - Accuracy, precision, recall, F1-score  
âœ… **Loss Tracking** - Visualize convergence over iterations  
âœ… **Complete ML Pipeline** - From data loading to predictions  

## ğŸ§® Mathematical Foundation

### Logistic Regression Equation
```
Ïƒ(z) = 1 / (1 + e^(-z))
```
Where:
- `z = w Â· x + b`
- `Ïƒ(z)` = sigmoid function (outputs probability between 0 and 1)
- `w` = weights
- `b` = bias

### Sigmoid Function
The sigmoid function maps any input to a probability between 0 and 1:
```
P(y=1|x) = Ïƒ(w Â· x + b)
```

### Loss Function (Cross-Entropy/Log Loss)
```
L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```
Where:
- `y` = true label (0 or 1)
- `Å·` = predicted probability

### Gradient Descent Updates
```
âˆ‚L/âˆ‚w = (1/n) Î£ (Å·áµ¢ - yáµ¢) Â· xáµ¢
âˆ‚L/âˆ‚b = (1/n) Î£ (Å·áµ¢ - yáµ¢)
```

### Update Rule
```
w := w - Î± Â· âˆ‚L/âˆ‚w
b := b - Î± Â· âˆ‚L/âˆ‚b
```
Where Î± is the learning rate.

## ğŸš€ Installation & Usage

### Requirements
```bash
pip install numpy pandas matplotlib seaborn
```

### Quick Start

```python
import numpy as np
import pandas as pd
from logistic_regression import LogisticRegression

# 1. Load and preprocess data
df = pd.read_csv('diabetes_dataset.csv')
df_clean = df.dropna().reset_index(drop=True)
df_shuffled = df_clean.sample(frac=1).reset_index(drop=True)

train = df_shuffled.iloc[:train_size]
test = df_shuffled.iloc[train_size:]

# 2. Extract features and labels
X_train = train.iloc[:, :-1].values.astype(float)
y_train = train.iloc[:, -1].values.astype(float)
X_test = test.iloc[:, :-1].values.astype(float)
y_test = test.iloc[:, -1].values.astype(float)

# 3. Scale features
X_mean = X_train.mean(axis=0)
X_std = X_train.std(axis=0)
X_train_scaled = (X_train - X_mean) / X_std
X_test_scaled = (X_test - X_mean) / X_std

# 4. Train model
model = LogisticRegression(learning_rate=0.01, iterations=1000)
model.fit(X_train_scaled, y_train)

# 5. Make predictions
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# 6. Evaluate
train_accuracy = np.mean(y_pred_train == y_train)
test_accuracy = np.mean(y_pred_test == y_test)

print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
```

## ğŸ“ Project Structure

```
Logistic_Regression_From_Numpy_Scratch/
â”‚
â”œâ”€â”€ diabetes_dataset.csv              # Dataset
â”œâ”€â”€ Logistic_regression.ipynb         # Main implementation
â”œâ”€â”€ LogisticRegression.py             # Model class
â”œâ”€â”€ README.md                         # This file
â””â”€â”€ visualizations/                   # Plots and figures
    â”œâ”€â”€ loss_curve.png
    â”œâ”€â”€ accuracy_comparison.png
    â””â”€â”€ feature_distributions.png
```

## ğŸ“ˆ Results

### Model Performance

| Metric | Value |
|--------|-------|
| **Training Accuracy** | ~78-85% |
| **Test Accuracy** | ~75-82% |
| **Convergence** | Stable after 300-400 iterations |
| **Loss Function** | Cross-Entropy / Log Loss |

## ğŸ“š Class Implementation

### `LogisticRegression`

```python
class LogisticRegression:
    def __init__(self, learning_rate=0.01, iterations=1000):
        """
        Initialize logistic regression
        
        Parameters:
        - learning_rate: Step size for gradient descent
        - iterations: Number of training iterations
        """
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None
        self.losses = []
    
    def sigmoid(self, z):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        """Train logistic regression using gradient descent"""
        # Initialize weights and bias
        # Iterate over epochs
        # Calculate cross-entropy loss
        # Update weights and bias
        
    def predict(self, X):
        """Make predictions on new data"""
        # Calculate probabilities using sigmoid
        # Convert to binary labels (0 or 1)
```

## ğŸ§  What I Learned

### 1. Mathematical Concepts
âœ… Logistic regression theory and sigmoid function  
âœ… Cross-entropy loss and its derivatives  
âœ… Gradient descent for probabilistic models  
âœ… Probability interpretation of model outputs  
âœ… Decision boundary in logistic regression  

### 2. Implementation Skills
âœ… NumPy operations for vectorized computations  
âœ… Feature scaling and normalization  
âœ… Sigmoid activation function implementation  
âœ… Gradient calculations for logistic loss  
âœ… Handling binary classification labels  

### 3. Machine Learning Fundamentals
âœ… Difference between regression and classification  
âœ… Probability thresholds for classification  
âœ… Model evaluation metrics (accuracy, precision, recall)  
âœ… Hyperparameter tuning (learning rate, iterations)  
âœ… Convergence monitoring via loss curves  

### 4. Data Preprocessing
âœ… Handling missing values  
âœ… Feature scaling importance  
âœ… Train-test split strategies  
âœ… Data shuffling and normalization  

### 5. Object-Oriented Programming
âœ… Encapsulation of model logic  
âœ… Reusable class design  
âœ… Clear separation of concerns  

## ğŸ“Š Visualizations

### 1. Training Loss Curve
Shows how cross-entropy loss decreases over iterations, indicating convergence.

```
Loss
â”‚
â”‚     â•±â•²
â”‚    â•±  â•²_______________
â”‚   â•±
â”‚  â•±
â”‚_â•±________________
 0        200      1000
       Iteration
```

**Interpretation:**
- Curve decreases â†’ Model learning correctly
- Plateau region â†’ Convergence achieved
- No divergence â†’ Stable training

### 2. Accuracy Comparison
Comparison of training vs test accuracy over iterations.

### 3. Feature Distributions
Histograms showing the distribution of different health features in the dataset.

## ğŸ”§ Hyperparameters

| Parameter | Default | Range | Effect |
|-----------|---------|-------|--------|
| **Learning Rate** | 0.01 | 0.001-0.1 | Step size in gradient descent |
| **Iterations** | 1000 | 100-5000 | Training epochs |
| **Decision Threshold** | 0.5 | 0.3-0.7 | Classification boundary |

## ğŸ“ Use Cases

This implementation can be used for:

- **Learning:** Understand classification fundamentals
- **Teaching:** Explain logistic regression to others
- **Medical Diagnosis:** Binary classification for disease prediction
- **Prototyping:** Quick model without dependencies
- **Customization:** Extend with regularization, multi-class support
- **Research:** Experiment with different optimizers

## ğŸ¤” Common Questions

**Q: Why implement from scratch?**  
A: To understand how logistic regression works mathematically and computationally.

**Q: When should I use this vs scikit-learn?**  
A: Use scikit-learn in production. Use this for learning and understanding.

**Q: How do I improve accuracy?**  
A: Try more iterations, adjust learning rate, scale features properly, or add polynomial features.

**Q: What is the sigmoid function?**  
A: It maps any value to a probability between 0 and 1, allowing logistic regression to output probabilities.

**Q: How is logistic regression different from linear regression?**  
A: Linear regression predicts continuous values. Logistic regression predicts probabilities for binary classification using the sigmoid function.

## ğŸ“ Key Concepts

### Sigmoid Function
- Maps continuous values to probabilities (0-1)
- S-shaped curve
- Used for binary classification

### Cross-Entropy Loss
- Measures difference between predicted probability and true label
- Zero loss when predictions are perfect
- Penalizes confident wrong predictions heavily

### Gradient Descent
- Iteratively updates weights to minimize loss
- Converges to optimal weights
- Learning rate controls step size

### Decision Boundary
- Threshold (typically 0.5) for classifying predictions
- Can be adjusted based on false positive/negative tradeoff

## ğŸ“Œ Important Notes

âš ï¸ **Feature Scaling:** Critical for convergence; always scale training data  
âš ï¸ **Data Leakage:** Fit scaler on training data only, then apply to test  
âš ï¸ **Learning Rate:** Too high â†’ divergence, too low â†’ slow convergence  
âš ï¸ **Imbalanced Data:** Consider class weights or different thresholds  
âš ï¸ **Label Format:** Ensure labels are 0 and 1, not other values  

## ğŸ† Project Achievements

âœ… Implemented complete logistic regression from scratch using only NumPy  
âœ… Achieved 75-82% accuracy on diabetes prediction  
âœ… Proper gradient descent with cross-entropy loss  
âœ… Clean OOP design with reusable class structure  
âœ… Comprehensive data preprocessing and feature scaling  
âœ… Multiple evaluation metrics and visualizations  
âœ… Mathematical rigor with proper gradient calculations  
âœ… Convergence monitoring and loss tracking  

## ğŸ‘¨â€ğŸ’» Author

**Your Name**  
First-year Engineering Student | Machine Learning Enthusiast  
GitHub: [[Your GitHub Profile]
](https://github.com/shaurya0702-droid)
## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- NumPy documentation for array operations
- Mathematical concepts from ML courses
- Diabetes dataset for real-world classification task
- Statistical learning literature on logistic regression

## ğŸ”— Related Topics

- Linear Regression (continuous prediction)
- Support Vector Machines (SVM)
- Neural Networks (extensions of logistic regression)
- Regularization (L1, L2)
- Multi-class Classification (one-vs-rest)

## ğŸ“ Questions?

Feel free to ask in GitHub Issues or reach out directly!

---

**Happy Learning! ğŸš€**

Last Updated: November 25, 2025  
Status: âœ… Complete and Working  
Test Accuracy: ~75-82%
